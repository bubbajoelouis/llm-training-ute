# llm-training-ute

The evolution of LLMs during training involves intricate interactions between data exposure, model architecture, and emergent properties. The Pythia suite \cite{biderman2023pythia} offers a controlled environment to study these dynamics with its 16 models ranging from 70M to 12B parameters, trained on public data in a consistent order. This paper introduces an extension of these studies into the domain of theoretical physics, using the Unified Theory of Energy (UTE) as a test framework.
